{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook For NLP (Natural Language Processing).\n",
    "<hr>\n",
    "\n",
    "* This Notebook is used for the process of social listening.\n",
    "* This notebook processes text (unstructured data)\n",
    "* Initially data brought from SentiOne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the parent directory of 'topology/'\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)  # Add 'parent/' to Python's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.m365.auth.microsoft_graph as mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio ya existe: C:/Users/Santi/Documents/atinna/repos/escuchas\\Antisemitism\\General\\2025-02-20_2025-05-19\\v2.0\n"
     ]
    }
   ],
   "source": [
    "from config import settings, paths\n",
    "from NLP_en import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Usando sesi√≥n existente para santiago.munera@atinna.co\n"
     ]
    }
   ],
   "source": [
    "# Autenticaci√≥n\n",
    "token_response = mg.authenticate(client_id= settings.id_client, auth= settings.AUTHORITY, scopes= settings.GRAPH_SCOPES)\n",
    "headers = mg.get_headers(token_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antisemitism/General/2025-02-20_2025-05-19/v2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{settings.dir_cliente}/{settings.dir_escucha}/{settings.dir_periodo}/{settings.dir_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_id = mg.get_team_id(headers, settings.dir_cliente) if headers else None\n",
    "sharepoint_site = mg.get_sharepoint_site(headers, team_id)\n",
    "site_id = sharepoint_site[\"id\"]\n",
    "drive_items = mg.get_drive_items(headers, site_id)\n",
    "dev_folder = next((file for file in drive_items if file[\"name\"].lower() == \"desarrollo\"), None)\n",
    "dev_folder_id = dev_folder[\"id\"]\n",
    "dev_files = mg.get_folder_contents(headers, site_id, dev_folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(paths.paths['local']['output']['lda']['base'], exist_ok=True)\n",
    "os.makedirs(paths.paths['local']['output']['lda']['models'], exist_ok=True)\n",
    "os.makedirs(paths.paths['local']['output']['lda']['figures'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Intentando crear 'General'...\n",
      "üìÇ La carpeta 'General' ya existe.\n",
      "üìÅ Intentando crear '2025-02-20_2025-05-19'...\n",
      "üìÇ La carpeta '2025-02-20_2025-05-19' ya existe.\n",
      "üìÅ Intentando crear 'v2.0'...\n",
      "üìÇ La carpeta 'v2.0' ya existe.\n",
      "üìÅ Intentando crear 'output'...\n",
      "üìÇ La carpeta 'output' ya existe.\n",
      "üìÅ Intentando crear 'lda'...\n",
      "üìÇ La carpeta 'lda' ya existe.\n"
     ]
    }
   ],
   "source": [
    "mg.create_folder(headers, site_id, dev_folder_id, paths.paths['cloud']['output']['lda']['base'], separator=\"/\")\n",
    "id_folder = mg.get_folder_id(headers, site_id, dev_folder_id, paths.paths['cloud']['output']['lda']['base'], separator=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Archivos en la carpeta Desarrollo:\n",
      "- AntiSemitismo_only_english.csv (01CVDJC6C4KPVXFOVD4JELNZUOGZ2JFCVO)\n"
     ]
    }
   ],
   "source": [
    "id_folder = mg.get_folder_id(headers, site_id, dev_folder_id, paths.paths['cloud']['data']['base'], separator=\"/\")\n",
    "files = mg.get_folder_contents(headers, site_id, id_folder)\n",
    "print(\"\\nüìÇ Archivos en la carpeta Desarrollo:\")\n",
    "for file in files:\n",
    "    print(f\"- {file['name']} ({file['id']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo descargado exitosamente en: C:/Users/Santi/Documents/atinna/repos/escuchas\\Antisemitism\\General\\2025-02-20_2025-05-19\\v2.0\\data\\AntiSemitismo_only_english.csv\n"
     ]
    }
   ],
   "source": [
    "file_name = input(\"Por favor digite el nombre exacto del archivo: \")\n",
    "file_metadata = next((file for file in files if file[\"name\"] == file_name), None)\n",
    "file_id = file_metadata[\"id\"]\n",
    "file = mg.download_file(headers, site_id, file_id,os.path.join(settings.temp_dir_path, \"data\"), create_missing_dirs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib='pandas'#polars, pandas\n",
    "\n",
    "#column names and column for use.\n",
    "names = {\n",
    "'Autor':'author',\n",
    "'Contenido de la publicaci√≥n':'content',#Minimum required\n",
    "'Creado':'date',\n",
    "'Contexto':'context',\n",
    "'Link para la fuente':'link',\n",
    "'Grupo de dominio':'domain_group',#Minimum required\n",
    "'Pa√≠s':'country',\n",
    "'Tipo espec√≠fico':'type'\n",
    "}\n",
    "\n",
    "# Filter of values for columns\n",
    "filter_list = ['Twitter','Facebook','Instagram','Video','TikTok','Reddit']\n",
    "\n",
    "new_stop = [\n",
    "    # art√≠culos, cuantificadores, conectores muy comunes\n",
    "    'a', 'an', 'the', 'and', 'or', 'but', 'if', 'so', 'yet', 'nor',\n",
    "    'for', 'to', 'of', 'in', 'on', 'at', 'by', 'off', 'from', 'with',\n",
    "    'without', 'into', 'onto', 'over', 'under', 'between', 'through',\n",
    "    'about', 'above', 'below', 'across', 'around', 'during', 'before',\n",
    "    'after', 'since', 'until', 'while', 'because', 'though', 'although',\n",
    "    'unless', 'where', 'when', 'whenever', 'wherever', 'once',\n",
    "\n",
    "    # pronombres, determinantes y sus variantes coloquiales\n",
    "    'i', 'me', 'my', 'mine', 'myself',\n",
    "    'you', 'u', 'ur', 'your', 'yours', 'yourself', 'yourselves',\n",
    "    'he', 'him', 'his', 'himself',\n",
    "    'she', 'her', 'hers', 'herself',\n",
    "    'it', 'its', 'itself',\n",
    "    'we', 'us', 'our', 'ours', 'ourselves',\n",
    "    'they', 'them', 'their', 'theirs', 'themselves',\n",
    "    'this', 'that', 'these', 'those', 'tho', 'thats', 'til',\n",
    "    'some', 'any', 'none', 'each', 'either', 'neither', 'both', 'all',\n",
    "    'another', 'other', 'others', 'such',\n",
    "\n",
    "    # verbos auxiliares y formas contra√≠das\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'being', 'been',\n",
    "    'do', 'does', 'did', 'doing',\n",
    "    'have', 'has', 'had', 'having',\n",
    "    'can', 'cant', \"can't\", 'cannot',\n",
    "    'could', 'couldnt', \"couldn't\",\n",
    "    'shall', 'should', 'shouldnt', \"shouldn't\",\n",
    "    'will', 'wont', \"won't\", 'would', 'wouldnt', \"wouldn't\",\n",
    "    'may', 'might', 'must', 'ought',\n",
    "    \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\",\n",
    "    \"i've\", \"you've\", \"we've\", \"they've\",\n",
    "    \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\",\n",
    "    \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\",\n",
    "    \"ain't\", \"aren't\", \"isn't\", \"wasn't\", \"weren't\", \"hasn't\",\n",
    "    \"haven't\", \"hadn't\", \"doesn't\", \"don't\", \"didn't\", \"won't\",\n",
    "    \"would've\", \"could've\", \"should've\", \"must've\", \"might've\",\n",
    "\n",
    "    # n√∫meros y cuantificadores escritos\n",
    "    'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',\n",
    "    'nine', 'ten', 'hundred', 'thousand', 'million', 'billion',\n",
    "    'first', 'second', 'third', 'fourth', 'fifth', 'last', 'next',\n",
    "    'many', 'few', 'several', 'lot', 'lots', 'plenty',\n",
    "\n",
    "    # adverbios y calificadores muy frecuentes / muletillas\n",
    "    'just', 'really', 'very', 'quite', 'pretty', 'rather', 'kinda',\n",
    "    'sorta', 'somehow', 'anyway', 'anyways', 'actually', 'basically',\n",
    "    'literally', 'seriously', 'especially', 'probably', 'likely',\n",
    "    'maybe', 'perhaps', 'almost', 'already', 'still', 'yet', 'ever',\n",
    "    'never', 'always', 'often', 'sometimes', 'rarely', 'seldom',\n",
    "    'soon', 'now', 'today', 'tonight', 'tomorrow', 'yesterday',\n",
    "    'ago', 'here', 'there', 'everywhere', 'elsewhere',\n",
    "\n",
    "    # interjecciones, filler words y coloquialismos\n",
    "    'oh', 'ok', 'okay', 'okey', 'yo', 'hey', 'hi', 'hello', 'bye',\n",
    "    'thanks', 'thank', 'thx', 'pls', 'plz', 'please', 'welcome',\n",
    "    'yeah', 'yea', 'yep', 'nope', 'nah', 'uh', 'uhh', 'uhm', 'umm',\n",
    "    'lol', 'lmao', 'rofl', 'omg', 'idk', 'imo', 'imho', 'btw', 'tho',\n",
    "    'haha', 'hehe', 'huh', 'wow', 'whoa', 'aw', 'aww', 'ouch', 'yikes',\n",
    "\n",
    "    # modismos de internet y abreviaturas\n",
    "    'brb', 'afaik', 'irl', 'fyi', 'ftw', 'gg', 'smh', 'tbh', 'tbqh',\n",
    "    'w/', 'w/o', 'b/c', 'bc', 'cuz', 'coz', 'cus', 'cause', 'ya',\n",
    "    'gonna', 'wanna', 'gotta', 'lemme', 'gimme', 'outta', 'lotta',\n",
    "    'kinda', 'sorta', 'shoulda', 'coulda', 'woulda', 'ain',\n",
    "    'n/a', 'n\\\\a', 'etc', 'etc.', 'et', 'al', 'vs', 'via',\n",
    "\n",
    "    # t√≠tulos y cortes√≠as\n",
    "    'mr', 'mrs', 'ms', 'miss', 'dr', 'prof', 'sir', 'madam', 'maam',\n",
    "    'lady', 'gent', 'gents',\n",
    "\n",
    "    # palabras gen√©ricas y de poco valor sem√°ntico\n",
    "    'thing', 'things', 'stuff', 'something', 'anything', 'nothing',\n",
    "    'everything', 'place', 'places', 'way', 'ways', 'kind', 'kinds',\n",
    "    'type', 'types', 'lot', 'lots', 'part', 'parts', 'bit', 'bits',\n",
    "    'area', 'areas', 'case', 'cases', 'point', 'points', 'side',\n",
    "    'sides', 'fact', 'facts', 'idea', 'ideas', 'problem', 'problems',\n",
    "    'question', 'questions',\n",
    "\n",
    "    # verbos comod√≠n (ya cubiertos en gran parte por stoplists est√°ndar,\n",
    "    # pero agregamos variaciones coloquiales)\n",
    "    'get', 'gets', 'getting', 'got', 'gotten',\n",
    "    'take', 'takes', 'taking', 'took',\n",
    "    'make', 'makes', 'making', 'made',\n",
    "    'say', 'says', 'saying', 'said',\n",
    "    'see', 'seen', 'seeing',\n",
    "    'come', 'comes', 'coming', 'came',\n",
    "    'go', 'goes', 'going', 'gone',\n",
    "    'know', 'knows', 'knowing', 'knew', 'known',\n",
    "    'think', 'thinks', 'thinking', 'thought',\n",
    "\n",
    "    # variaciones ortogr√°ficas o con ap√≥strofes omitidos (SMS/chat)\n",
    "    'dont', 'doesnt', 'isnt', 'wasnt', 'werent', 'hasnt', 'havent',\n",
    "    'hadnt', 'couldnt', 'shouldnt', 'wouldnt', 'cant', 'wont', 'im',\n",
    "    'ive', 'ill', 'id', 'youre', 'youve', 'youll', 'youd', 'theyre',\n",
    "    'theyve', 'theyll', 'theyd', 'weve', 'well', 'wed', 'hes', 'shes',\n",
    "    'its', 'thats', 'theres', 'heres', 'whats', 'lets',\n",
    "\n",
    "    # nombres propios muy gen√©ricos (opcionales, pero a veces se filtran)\n",
    "    'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday',\n",
    "    'sunday', 'january', 'february', 'march', 'april', 'may', 'june',\n",
    "    'july', 'august', 'september', 'october', 'november', 'december',\n",
    "    'usa', 'us', 'u.s.', 'uk', 'u.k.', 'eu', 'e.u.', 'america',\n",
    "\n",
    "    # tokens miscel√°neos y de puntuaci√≥n\n",
    "    '.', ',', ';', ':', '!', '?', '‚Ä¶', '-', '‚Äì', '‚Äî', '(', ')',\n",
    "    '[', ']', '{', '}', '¬´', '¬ª', '\"', \"'\", '`', '¬¥', '‚Äú', '‚Äù', '‚Äò', '‚Äô',\n",
    "    '&', '%', '$', '#', '@', '¬©', '¬Æ', '‚Ñ¢', '+', '=', '/', '\\\\'\n",
    "]\n",
    "\n",
    "\n",
    "sample_frac=None#0.25\n",
    "\n",
    "n_volume=None#10000\n",
    "\n",
    "optimization = True\n",
    "\n",
    "min_topic = 4#2\n",
    "\n",
    "max_topic = 10#2\n",
    "\n",
    "k=None\n",
    "\n",
    "sep=';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:(821056, 44)\n",
      "columns :Index(['id', 'Tipo espec√≠fico', 'T√≠tulo', 'Autor', 'Id del Author',\n",
      "       'Contenido de la publicaci√≥n', 'Creado', 'Agregar al sistema',\n",
      "       'Contexto', 'Link para la fuente', 'Dominio', 'Sentimiento',\n",
      "       'Puntos de sentimiento', 'Grupo de dominio', 'Etiqueta',\n",
      "       'Palabras Clave', 'G√©nero', 'Nombre del proyecto', 'Unnamed: 18',\n",
      "       'influenceScore', 'comments', 'views', 'shares', 'wow', 'love', 'like',\n",
      "       'haha', 'sad', 'angry', 'thankful', 'uniqueVievs', 'fans',\n",
      "       'Categor√≠a de p√°gina de Facebook', 'retweet', 'favs', 'hearts', 'likes',\n",
      "       'dislikes', 'followers', 'Geolocalizaci√≥n', 'Lenguaje', 'Pa√≠s',\n",
      "       'Calificaci√≥n', 'ID del hilo'],\n",
      "      dtype='object')\n",
      "Available social networks: ['Portales' 'Twitter' 'Facebook' 'Forums' 'Reddit' 'Blogs' 'Rese√±as']\n",
      "CPU times: total: 17.3 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#the format of the data can changue. A firs view for selectin parameters.\n",
    "answer = input('Do you require the first description of the data?? yes or NO')\n",
    "if answer.lower()=='yes':\n",
    "    df = local_reading(path=file,sep=sep,lib=lib)\n",
    "    x = df['Grupo de dominio'].unique()\n",
    "    print(f'Data size:{df.shape}')\n",
    "    print(f'columns :{df.columns}')\n",
    "    print(f'Available social networks: {x}')\n",
    "    del df, x, answer\n",
    "else:\n",
    "    print('No need for the initial description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Process LDA\n",
      "Stopwords have been downloaded.\n",
      "*Reading\n",
      "Size Of the Data:(821056, 44)\n",
      "Reading --- 17.08980894088745 seconds ---\n",
      "*Normalize columns\n",
      "Is pandas Dataframe\n",
      "Normalize columns --- 0.17403364181518555 seconds ---\n",
      "*Network distribution\n",
      "Network distribution --- 0.06351327896118164 seconds ---\n",
      "*Drop nan\n",
      "Is pandas Dataframe\n",
      "Drop nan --- 0.10552978515625 seconds ---\n",
      "*Repeated post\n",
      "Repeated post --- 15.310202598571777 seconds ---\n",
      "*Hash tags table\n",
      "Hash tags table --- 0.6215035915374756 seconds ---\n",
      "*Drop duplicates\n",
      "Is pandas Dataframe\n",
      "Drop duplicates --- 0.238602876663208 seconds ---\n",
      "Size Of the Data after cleaning:(754655, 8)\n",
      "*Set data types\n",
      "Set data types --- 0.18492865562438965 seconds ---\n",
      "*Filter columns of social network\n",
      "['Twitter' 'Facebook' 'Reddit']\n",
      "Filter columns of social network --- 0.7569177150726318 seconds ---\n",
      "*Text cleaning paralelized\n",
      "Is pandas Dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Santi\\miniconda3\\envs\\social-analysis\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning paralelized --- 51.669341802597046 seconds ---\n",
      "*Transform list into string\n",
      "Is pandas Dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Santi\\miniconda3\\envs\\social-analysis\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from list to str --- 69.17955803871155 seconds ---\n",
      "*Drop duplicates after cleaning\n",
      "Is pandas Dataframe\n",
      "Drop duplicates after cleaning --- 0.33431100845336914 seconds ---\n",
      "Size Of the Data after cleaning:(532824, 9)\n",
      "*From list to string\n",
      "Is pandas Dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Santi\\miniconda3\\envs\\social-analysis\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from str to list --- 19.219505310058594 seconds ---\n",
      "*Bigram instance\n",
      "Bigram instance --- 28.28022074699402 seconds ---\n",
      "*text procesing\n",
      "Is pandas Dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Santi\\miniconda3\\envs\\social-analysis\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text procesing --- 361.7792537212372 seconds ---\n",
      "*Modeling LDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LDA different k: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [2:16:42<00:00, 1367.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Modeling LDA --- 8218.826862335205 seconds ---\n",
      "*Joing words\n",
      "Is pandas Dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Santi\\miniconda3\\envs\\social-analysis\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joing words --- 12.978819131851196 seconds ---\n",
      "End of process --- 8750.989723920822 seconds ---\n",
      "None\n",
      "CPU times: total: 2h 21min 15s\n",
      "Wall time: 2h 25min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Social_listening_data_process_and_modeling_LDA(\n",
    "    sep=sep,\n",
    "    path_data=file,\n",
    "    path_output= paths.paths['local']['output']['lda']['base'],\n",
    "    path_output_model= paths.paths['local']['output']['lda']['models'],\n",
    "    lib=lib,\n",
    "    column_names=names,\n",
    "    filter_list=filter_list,\n",
    "    new_stop=new_stop,\n",
    "    sample_frac=sample_frac,\n",
    "    n_volume=n_volume,\n",
    "    optimization =optimization,\n",
    "    min_topics=min_topic, \n",
    "    max_topics=max_topic,\n",
    "    step_size=1,\n",
    "    k=k,\n",
    "    n_cpu=6,\n",
    "    n_cpu_lda=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "Contains information about the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Santi/Documents/atinna/repos/escuchas\\\\Antisemitism\\\\General\\\\2025-02-20_2025-05-19\\\\v2.0\\\\lda\\\\metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path_metadata \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(paths\u001b[38;5;241m.\u001b[39mpaths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mjson_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m metadata\n",
      "File \u001b[1;32mc:\\Users\\Santi\\Documents\\atinna\\repos\\social-analysis\\lda\\NLP_en.py:1058\u001b[0m, in \u001b[0;36mjson_read\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjson_read\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read Json file\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \n\u001b[0;32m   1046\u001b[0m \u001b[38;5;124;03m    Read Json file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;124;03m    Raise\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f_in)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Santi/Documents/atinna/repos/escuchas\\\\Antisemitism\\\\General\\\\2025-02-20_2025-05-19\\\\v2.0\\\\lda\\\\metadata.json'"
     ]
    }
   ],
   "source": [
    "path_metadata = os.path.join(paths.paths['local']['output']['lda']['base'],\"metadata.json\")\n",
    "metadata = json_read(path_metadata)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hash_global_pandas.csv']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in os.listdir(paths.paths['local']['output']['lda']['base']) if 'Hash'in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash= [local_reading(path=paths.paths['local']['output']['lda']['base']+i) for i in os.listdir(paths.paths['local']['output']['lda']['base']) if 'Hash'in i]\n",
    "hash[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most repeated posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repe= [local_reading(path=paths['local']['output']['base']+i) for i in os.listdir(paths['local']['output']['base']) if 'Repea'in i]\n",
    "# df_repe = Repe[0].head(15).set_index('content')\n",
    "# df_repe [df_repe['count']>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Netw= [local_reading(path=paths.paths['local']['output']['lda']['base']+i) for i in os.listdir(paths.paths['local']['output']['lda']['base']) if 'net'in i]\n",
    "Netw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Usuario no autenticado o token expirado.\n"
     ]
    }
   ],
   "source": [
    "mg.upload_folder(headers, site_id, dev_folder_id, paths.paths['local']['output']['lda']['base'], paths.paths['cloud']['output']['lda']['base'], separator=\"/\", conflict_behavior=\"fail\", skip_on_error=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
